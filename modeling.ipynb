{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Recommendation Model\n",
    "\n",
    "## Purpose\n",
    "This Python notebook is dedicated to building and training the machine learning model used as the backend for the Restaurant Recommendation System. The goal is to create a robust recommendation system that suggests restaurants based on user preferences, historical data, and other relevant features.\n",
    "\n",
    "## Frameworks Used\n",
    "Built with sci.kit learn and XGBoost. The GradientBoostingRegressor was used to build this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Relevant Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name         city state  postal_code   latitude  \\\n",
      "0              Metro Grille     Flanders    NJ         7836  39.949904   \n",
      "1  Helen's Cafe and Gardens      Alloway    NJ         8001  39.563830   \n",
      "2       Alloway Village Inn      Alloway    NJ         8001  39.555717   \n",
      "3                McDonald's  Cherry Hill    NJ         8002  39.936079   \n",
      "4              Little Tokyo  Cherry Hill    NJ         8002  39.943728   \n",
      "\n",
      "   longitude  stars  review_count  \\\n",
      "0 -75.161599    3.0            17   \n",
      "1 -75.363824    4.0             5   \n",
      "2 -75.360766    3.5            13   \n",
      "3 -75.044117    2.5            18   \n",
      "4 -75.026066    4.0            12   \n",
      "\n",
      "                                          categories  B01001_001E  ...  \\\n",
      "0          Restaurants, Asian Fusion, American (New)        12034  ...   \n",
      "1  Restaurants, Cafes, Breakfast & Brunch, Venues...          835  ...   \n",
      "2  American (Traditional), Restaurants, Bars, Nig...          835  ...   \n",
      "3  Burgers, Food, Restaurants, Fast Food, Coffee ...        23630  ...   \n",
      "4                  Japanese, Sushi Bars, Restaurants        23630  ...   \n",
      "\n",
      "   S1903_C03_032E  S1903_C03_033E  S1903_C03_034E  S1903_C03_035E  \\\n",
      "0          182798          241500           75721           42083   \n",
      "1          136579               -           69773               -   \n",
      "2          136579               -           69773               -   \n",
      "3          126982          161806           53500           48480   \n",
      "4          126982          161806           53500           48480   \n",
      "\n",
      "   S1903_C03_036E  S1903_C03_037E  S1903_C03_038E  S1903_C03_039E  \\\n",
      "0           42083               -          100134          100938   \n",
      "1               -               -               -               -   \n",
      "2               -               -               -               -   \n",
      "3           47049          100698           63438           40616   \n",
      "4           47049          100698           63438           40616   \n",
      "\n",
      "   S1903_C03_040E  star_count  \n",
      "0               -        51.0  \n",
      "1               -        20.0  \n",
      "2               -        45.5  \n",
      "3          136477        45.0  \n",
      "4          136477        48.0  \n",
      "\n",
      "[5 rows x 154 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/xb2bmr4d5718m4gjtrxd8h2w0000gn/T/ipykernel_37142/2725881843.py:2: DtypeWarning: Columns (116,117,123,124,125,127,128,129,130,132,133,139,140,145,146,147,149,150,153,154) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  complete = pd.read_csv(\"./data/restaurantdemo.csv\") #Directory\n"
     ]
    }
   ],
   "source": [
    "#Read Pandas Dataframe\n",
    "complete = pd.read_csv(\"./data/restaurantdemo.csv\") #Directory\n",
    "complete = complete.drop(columns=['business_id', 'GEO_ID', 'address'])\n",
    "\n",
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sk.Learn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sk.Learn Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split Setup\n",
    "y = complete['stars']\n",
    "X = complete.drop(columns=['star_count', 'stars'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Columns\n",
    "text_features = ['name', 'city', 'state', 'categories']\n",
    "\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "#TF-IDF\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_features),\n",
    "        ('name', TfidfVectorizer(), 'name'),\n",
    "        ('city', TfidfVectorizer(), 'city'),\n",
    "        ('categories', TfidfVectorizer(), 'categories'),\n",
    "    ])\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('regressor', gb_model)])\n",
    "\n",
    "#Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Evaluate\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Testing via HuggingFace\n",
    "Using batch processing to tokenize and find embeddings for each of the text-based columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihir/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mihir/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Columns\n",
    "text_column = ['name', 'city', 'state', 'categories']\n",
    "\n",
    "# Batch Computations\n",
    "batch_size = 100  # 100 Rows Per Call\n",
    "num_batches = (len(complete) + batch_size - 1) // batch_size\n",
    "\n",
    "# Store Embeddings\n",
    "bert_embeddings_list = []\n",
    "\n",
    "# Loop Through Each Batch and Store\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "\n",
    "    # Tokenize\n",
    "    batch_texts = list(complete[text_column].iloc[start_idx:end_idx])\n",
    "    tokenized = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "\n",
    "    # Extract\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Convert\n",
    "    bert_embeddings_list.append(embeddings.numpy())\n",
    "\n",
    "# Concatenate\n",
    "concatenated_embeddings = np.concatenate(bert_embeddings_list, axis=0)\n",
    "\n",
    "# New Columns\n",
    "new_column_names = [f'{text_column}_bert_embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "\n",
    "# DF with Embeddings\n",
    "bert_embeddings_df = pd.DataFrame(concatenated_embeddings, columns=new_column_names)\n",
    "\n",
    "# Concatenate in PD\n",
    "complete = pd.concat([complete, bert_embeddings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data with Embeddings\n",
    "complete.to_csv('./data/embedded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all object columns to numeric\n",
    "for col in complete.select_dtypes(include='object').columns:\n",
    "    complete[col] = pd.to_numeric(complete[col], errors='coerce')\n",
    "\n",
    "\n",
    "complete.drop(columns=['star_count', 'name', 'city', 'state',])\n",
    "complete.to_csv('./data/modelready.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name  city  state  postal_code   latitude  longitude  stars  review_count  \\\n",
      "0   NaN   NaN    NaN         7836  39.949904 -75.161599    3.0            17   \n",
      "1   NaN   NaN    NaN         8001  39.563830 -75.363824    4.0             5   \n",
      "2   NaN   NaN    NaN         8001  39.555717 -75.360766    3.5            13   \n",
      "3   NaN   NaN    NaN         8002  39.936079 -75.044117    2.5            18   \n",
      "4   NaN   NaN    NaN         8002  39.943728 -75.026066    4.0            12   \n",
      "\n",
      "   categories  B01001_001E  ...  S1903_C03_032E  S1903_C03_033E  \\\n",
      "0         NaN        12034  ...        182798.0        241500.0   \n",
      "1         NaN          835  ...        136579.0             NaN   \n",
      "2         NaN          835  ...        136579.0             NaN   \n",
      "3         NaN        23630  ...        126982.0        161806.0   \n",
      "4         NaN        23630  ...        126982.0        161806.0   \n",
      "\n",
      "   S1903_C03_034E  S1903_C03_035E  S1903_C03_036E  S1903_C03_037E  \\\n",
      "0         75721.0         42083.0         42083.0             NaN   \n",
      "1         69773.0             NaN             NaN             NaN   \n",
      "2         69773.0             NaN             NaN             NaN   \n",
      "3         53500.0         48480.0         47049.0        100698.0   \n",
      "4         53500.0         48480.0         47049.0        100698.0   \n",
      "\n",
      "   S1903_C03_038E  S1903_C03_039E  S1903_C03_040E  star_count  \n",
      "0        100134.0        100938.0             NaN        51.0  \n",
      "1             NaN             NaN             NaN        20.0  \n",
      "2             NaN             NaN             NaN        45.5  \n",
      "3         63438.0         40616.0        136477.0        45.0  \n",
      "4         63438.0         40616.0        136477.0        48.0  \n",
      "\n",
      "[5 rows x 154 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove Invalid Column Names\n",
    "original_columns = complete.columns\n",
    "replacement_mapping = {col: col.replace('[', '_').replace(']', '_') for col in original_columns if '_bert_embedding_' in col}\n",
    "complete.rename(columns=replacement_mapping, inplace=True)\n",
    "\n",
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split Setup\n",
    "y = complete['stars'] # just stars column\n",
    "X = complete.drop(columns=['stars']) # df with no stars column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "bst = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "\n",
    "# Fit\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0020993842692766033\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
