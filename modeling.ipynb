{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Recommendation Model\n",
    "\n",
    "## Purpose\n",
    "This Python notebook is dedicated to building and training the machine learning model used as the backend for the Restaurant Recommendation System. The goal is to create a robust recommendation system that suggests restaurants based on user preferences, historical data, and other relevant features.\n",
    "\n",
    "## Frameworks Used\n",
    "Built with sci.kit learn and XGBoost. The GradientBoostingRegressor was used to build this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Relevant Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       name         city state  postal_code   latitude  \\\n",
      "0              Metro Grille     Flanders    NJ         7836  39.949904   \n",
      "1  Helen's Cafe and Gardens      Alloway    NJ         8001  39.563830   \n",
      "2       Alloway Village Inn      Alloway    NJ         8001  39.555717   \n",
      "3                McDonald's  Cherry Hill    NJ         8002  39.936079   \n",
      "4              Little Tokyo  Cherry Hill    NJ         8002  39.943728   \n",
      "\n",
      "   longitude                                         categories  B01001_001E  \\\n",
      "0 -75.161599          Restaurants, Asian Fusion, American (New)        12034   \n",
      "1 -75.363824  Restaurants, Cafes, Breakfast & Brunch, Venues...          835   \n",
      "2 -75.360766  American (Traditional), Restaurants, Bars, Nig...          835   \n",
      "3 -75.044117  Burgers, Food, Restaurants, Fast Food, Coffee ...        23630   \n",
      "4 -75.026066                  Japanese, Sushi Bars, Restaurants        23630   \n",
      "\n",
      "   B01001_002E  B01001_003E  ...  S1903_C03_032E  S1903_C03_033E  \\\n",
      "0         5893          355  ...          182798          241500   \n",
      "1          420            0  ...          136579               -   \n",
      "2          420            0  ...          136579               -   \n",
      "3        11866         1068  ...          126982          161806   \n",
      "4        11866         1068  ...          126982          161806   \n",
      "\n",
      "   S1903_C03_034E  S1903_C03_035E  S1903_C03_036E  S1903_C03_037E  \\\n",
      "0           75721           42083           42083               -   \n",
      "1           69773               -               -               -   \n",
      "2           69773               -               -               -   \n",
      "3           53500           48480           47049          100698   \n",
      "4           53500           48480           47049          100698   \n",
      "\n",
      "   S1903_C03_038E  S1903_C03_039E  S1903_C03_040E  star_count  \n",
      "0          100134          100938               -        51.0  \n",
      "1               -               -               -        20.0  \n",
      "2               -               -               -        45.5  \n",
      "3           63438           40616          136477        45.0  \n",
      "4           63438           40616          136477        48.0  \n",
      "\n",
      "[5 rows x 152 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6l/xb2bmr4d5718m4gjtrxd8h2w0000gn/T/ipykernel_39314/1322297738.py:2: DtypeWarning: Columns (116,117,123,124,125,127,128,129,130,132,133,139,140,145,146,147,149,150,153,154) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  complete = pd.read_csv(\"./data/restaurantdemo.csv\") #Directory\n"
     ]
    }
   ],
   "source": [
    "#Read Pandas Dataframe\n",
    "complete = pd.read_csv(\"./data/restaurantdemo.csv\") #Directory\n",
    "complete = complete.drop(columns=['business_id', 'GEO_ID', 'address', 'stars', 'review_count'])\n",
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sk.Learn Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF to each text column separately\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "for col in ['name', 'city', 'state', 'categories']:\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(complete[col])\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    complete = pd.concat([complete, tfidf_df], axis=1)\n",
    "\n",
    "# Drop the original text columns\n",
    "complete = complete.drop(['name', 'city', 'state', 'categories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   postal_code   latitude  longitude  B01001_001E  B01001_002E  B01001_003E  \\\n",
      "0         7836  39.949904 -75.161599        12034         5893          355   \n",
      "1         8001  39.563830 -75.363824          835          420            0   \n",
      "2         8001  39.555717 -75.360766          835          420            0   \n",
      "3         8002  39.936079 -75.044117        23630        11866         1068   \n",
      "4         8002  39.943728 -75.026066        23630        11866         1068   \n",
      "\n",
      "   B01001_004E  B01001_005E  B01001_006E  B01001_007E  ...  wineries  wings  \\\n",
      "0          267          586           62          174  ...       0.0    0.0   \n",
      "1            0           33            9            5  ...       0.0    0.0   \n",
      "2            0           33            9            5  ...       0.0    0.0   \n",
      "3          752          650          422          245  ...       0.0    0.0   \n",
      "4          752          650          422          245  ...       0.0    0.0   \n",
      "\n",
      "   women  wraps  yelp  yoga  yogurt  your  yourself  zoos  \n",
      "0    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "1    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "2    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "3    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "4    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "\n",
      "[5 rows x 18009 columns]\n"
     ]
    }
   ],
   "source": [
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   postal_code  latitude  longitude  B01001_001E  B01001_002E  B01001_003E  \\\n",
      "0     0.000000  0.764098   0.989054     0.120581     0.120460     0.086228   \n",
      "1     0.001868  0.740280   0.984602     0.008367     0.008585     0.000000   \n",
      "2     0.001868  0.739780   0.984669     0.008367     0.008585     0.000000   \n",
      "3     0.001879  0.763246   0.991641     0.236774     0.242554     0.259412   \n",
      "4     0.001879  0.763717   0.992038     0.236774     0.242554     0.259412   \n",
      "\n",
      "   B01001_004E  B01001_005E  B01001_006E  B01001_007E  ...  wineries  wings  \\\n",
      "0     0.067732     0.148204     0.025931     0.038275  ...       0.0    0.0   \n",
      "1     0.000000     0.008346     0.003764     0.001100  ...       0.0    0.0   \n",
      "2     0.000000     0.008346     0.003764     0.001100  ...       0.0    0.0   \n",
      "3     0.190766     0.164390     0.176495     0.053894  ...       0.0    0.0   \n",
      "4     0.190766     0.164390     0.176495     0.053894  ...       0.0    0.0   \n",
      "\n",
      "   women  wraps  yelp  yoga  yogurt  your  yourself  zoos  \n",
      "0    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "1    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "2    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "3    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "4    0.0    0.0   0.0   0.0     0.0   0.0       0.0   0.0  \n",
      "\n",
      "[5 rows x 18009 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalize all columns\n",
    "complete = complete.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "for col in complete.columns:\n",
    "    col_min = complete[col].min()\n",
    "    col_range = complete[col].max() - col_min\n",
    "    complete[col] = (complete[col] - col_min) / col_range\n",
    "\n",
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = complete.loc[:, ~complete.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.010804586658382947\n"
     ]
    }
   ],
   "source": [
    "# Assume 'stars' is the column you want to predict\n",
    "y = complete['star_count']\n",
    "\n",
    "# Drop the target column from the features\n",
    "X = complete.drop('star_count', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model setup\n",
    "bst = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Model training\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the test set\n",
    "predictions = bst.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sk.Learn Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split Setup\n",
    "y = complete['stars']\n",
    "X = complete.drop(columns=['stars'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.43657840168681056\n"
     ]
    }
   ],
   "source": [
    "# Text Columns\n",
    "text_features = ['name', 'city', 'state', 'categories']\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Testing via HuggingFace\n",
    "Using batch processing to tokenize and find embeddings for each of the text-based columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihir/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mihir/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Columns\n",
    "text_column = ['name', 'city', 'state', 'categories']\n",
    "\n",
    "# Batch Computations\n",
    "batch_size = 100  # 100 Rows Per Call\n",
    "num_batches = (len(complete) + batch_size - 1) // batch_size\n",
    "\n",
    "# Store Embeddings\n",
    "bert_embeddings_list = []\n",
    "\n",
    "# Loop Through Each Batch and Store\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = (i + 1) * batch_size\n",
    "\n",
    "    # Tokenize\n",
    "    batch_texts = list(complete[text_column].iloc[start_idx:end_idx])\n",
    "    tokenized = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized)\n",
    "\n",
    "    # Extract\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Convert\n",
    "    bert_embeddings_list.append(embeddings.numpy())\n",
    "\n",
    "# Concatenate\n",
    "concatenated_embeddings = np.concatenate(bert_embeddings_list, axis=0)\n",
    "\n",
    "# New Columns\n",
    "new_column_names = [f'{text_column}_bert_embedding_{i}' for i in range(embeddings.shape[1])]\n",
    "\n",
    "# DF with Embeddings\n",
    "bert_embeddings_df = pd.DataFrame(concatenated_embeddings, columns=new_column_names)\n",
    "\n",
    "# Concatenate in PD\n",
    "complete = pd.concat([complete, bert_embeddings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Data with Embeddings\n",
    "complete.to_csv('./data/embedded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all object columns to numeric\n",
    "for col in complete.select_dtypes(include='object').columns:\n",
    "    complete[col] = pd.to_numeric(complete[col], errors='coerce')\n",
    "\n",
    "\n",
    "complete = complete.drop(columns=['name', 'city', 'state'])\n",
    "#complete.to_csv('./data/modelready.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   postal_code   latitude  longitude  stars  review_count  categories  \\\n",
      "0         7836  39.949904 -75.161599    3.0            17         NaN   \n",
      "1         8001  39.563830 -75.363824    4.0             5         NaN   \n",
      "2         8001  39.555717 -75.360766    3.5            13         NaN   \n",
      "3         8002  39.936079 -75.044117    2.5            18         NaN   \n",
      "4         8002  39.943728 -75.026066    4.0            12         NaN   \n",
      "\n",
      "   B01001_001E  B01001_002E  B01001_003E  B01001_004E  ...  \\\n",
      "0        12034         5893          355          267  ...   \n",
      "1          835          420            0            0  ...   \n",
      "2          835          420            0            0  ...   \n",
      "3        23630        11866         1068          752  ...   \n",
      "4        23630        11866         1068          752  ...   \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_758  \\\n",
      "0                                           0.228715            \n",
      "1                                           0.080919            \n",
      "2                                           0.128508            \n",
      "3                                           0.144302            \n",
      "4                                           0.228715            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_759  \\\n",
      "0                                           0.076724            \n",
      "1                                          -0.174918            \n",
      "2                                          -0.181380            \n",
      "3                                          -0.105592            \n",
      "4                                           0.076724            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_760  \\\n",
      "0                                           0.135232            \n",
      "1                                           0.197364            \n",
      "2                                           0.343376            \n",
      "3                                           0.263701            \n",
      "4                                           0.135232            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_761  \\\n",
      "0                                           0.287441            \n",
      "1                                           0.139538            \n",
      "2                                           0.195926            \n",
      "3                                           0.126444            \n",
      "4                                           0.287441            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_762  \\\n",
      "0                                           0.003794            \n",
      "1                                           0.184262            \n",
      "2                                           0.116750            \n",
      "3                                           0.066586            \n",
      "4                                           0.003794            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_763  \\\n",
      "0                                          -0.265133            \n",
      "1                                          -0.184703            \n",
      "2                                          -0.062069            \n",
      "3                                          -0.192762            \n",
      "4                                          -0.265133            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_764  \\\n",
      "0                                           0.249168            \n",
      "1                                           0.037747            \n",
      "2                                           0.117361            \n",
      "3                                           0.145028            \n",
      "4                                           0.249168            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_765  \\\n",
      "0                                          -0.242281            \n",
      "1                                          -0.320912            \n",
      "2                                          -0.135174            \n",
      "3                                          -0.262136            \n",
      "4                                          -0.242281            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_766  \\\n",
      "0                                           0.018291            \n",
      "1                                           0.038248            \n",
      "2                                          -0.078362            \n",
      "3                                           0.096196            \n",
      "4                                           0.018291            \n",
      "\n",
      "   _'name', 'city', 'state', 'categories'__bert_embedding_767  \n",
      "0                                           0.381550           \n",
      "1                                           0.289199           \n",
      "2                                           0.380558           \n",
      "3                                           0.337230           \n",
      "4                                           0.381550           \n",
      "\n",
      "[5 rows x 918 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove Invalid Column Names\n",
    "original_columns = complete.columns\n",
    "replacement_mapping = {col: col.replace('[', '_').replace(']', '_') for col in original_columns if '_bert_embedding_' in col}\n",
    "complete.rename(columns=replacement_mapping, inplace=True)\n",
    "\n",
    "print(complete.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split Setup\n",
    "y = complete['stars'] # just stars column\n",
    "X = complete.drop(columns=['stars']) # df with no stars column\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task with squared error loss\n",
    "    'learning_rate': 0.1,              # Step size shrinkage to prevent overfitting\n",
    "    'max_depth': 2,                    # Maximum depth of a tree\n",
    "    'subsample': 0.8,                  # Fraction of samples used for training each tree\n",
    "    'colsample_bytree': 0.8,           # Fraction of features used for training each tree\n",
    "    'n_estimators': 100,               # Number of boosting rounds (trees)\n",
    "    'random_state': 42                  # Random seed for reproducibility\n",
    "}\n",
    "\n",
    "# Model\n",
    "bst = xgb.XGBRegressor(**params)\n",
    "\n",
    "# Fit\n",
    "bst.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = bst.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 0.631971720180573\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_test, y_test are your testing data\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error on Test Set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.6343937692518058\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "print(f'Mean Absolute Error: {mae}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
